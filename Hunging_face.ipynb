{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba214e3f",
   "metadata": {},
   "source": [
    "# Fondamentaux de la bibliothèque Transformers\n",
    "\n",
    "Hugging Face repose sur trois blocs fondamentaux qui permettent de passer d'un texte brut à un modèle capable de prédire ou de générer du contenu.\n",
    "\n",
    "---\n",
    "\n",
    "## Les modèles\n",
    "Le choix du modèle dépend de la tâche finale. La bibliothèque propose des classes automatisées pour charger les bonnes architectures comme le montre exemple ce dessous.\n",
    "\n",
    "```python\n",
    "from transformers import AutoModel\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import AutoModelForSeq2SeqLM\n",
    "```\n",
    "---\n",
    "\n",
    "## Les tokenizers\n",
    "Un modèle ne lit pas de texte. Il lit des nombres. La classe AutoTokenizer s'occupe de cette conversion. Elle segmente le texte en unités appelées tokens et les transforme en vecteurs numériques. C'est l'étape obligatoire avant toute injection de donnée dans le modèle.\n",
    "\n",
    "```python\n",
    "from transformers import AutoTokenizer\n",
    "```\n",
    "\n",
    "## L’entraînement\n",
    "La boucle d'apprentissage est gérée par la classe Trainer. Elle évite d'écrire manuellement les fonctions de perte ou les mises à jour de poids.\n",
    "\n",
    "* Trainer : Le Trainer standard convient aux tâches classiques  comme la classification de texte (BERT, RoBERTa…)\n",
    "* Seq2SeqTrainer: est optimisé pour la génération de texte.\n",
    "* TrainingArguments: Les TrainingArguments et Seq2SeqTrainingArguments permettent de configurer les hyperparamètres comme la vitesse d'apprentissage ou le nombre d'itérations.\n",
    "\n",
    "## Résumé simple\n",
    "\n",
    "| Cas                       | Classe                                           |\n",
    "| ------------------------- | ------------------------------------------------ |\n",
    "| Classification            | `Trainer`                                        |\n",
    "| Traduction / Résumé       | `Seq2SeqTrainer`                                 |\n",
    "| Paramètres d'entraînement | `TrainingArguments` / `Seq2SeqTrainingArguments` |\n",
    "\n",
    "## Remarque\n",
    "\n",
    "Le Trainer est une couche simplifiée. Il reste tout à fait possible d'utiliser PyTorch de manière brute avec une boucle for et un optimiseur classique, mais le Trainer réduit drastiquement le risque d'erreur et le volume de code nécessaire."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47d5229d",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
